# -*- coding: utf-8 -*-
"""SocialAnayst.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D_m70GK_o8xbk_qeYFHATgSp6yZ-LS5N
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from datetime import datetime
import joblib

# Load dataset
df= pd.read_csv("/content/updated_twitter_accounts.csv")
print(df.head())

df = df.drop("url", axis=1)
df['created_at'] = pd.to_datetime(df['created_at'], format='%a %b %d %H:%M:%S +0000 %Y', utc=True)
reference_date = pd.Timestamp('2015-02-14', tz='UTC')
df['account_age'] = (reference_date - df['created_at']).dt.days

# Plot account age distribution
plt.figure(figsize=(10, 6))
sns.histplot(df['account_age'], bins=30, kde=True, color='blue')
plt.title('Distribution of Account Ages')
plt.xlabel('Account Age (Days)')
plt.ylabel('Frequency')
plt.show()

# Compute follower-to-following ratio
df['follower_to_following_ratio'] = df['followers_count'] / (df['friends_count'] + 1)

# Plot follower-to-following ratio distribution
plt.figure(figsize=(10, 6))
sns.histplot(df['follower_to_following_ratio'], bins=30, kde=True, color='green')
plt.title('Follower-to-Following Ratio Distribution')
plt.xlabel('Follower-to-Following Ratio')
plt.ylabel('Frequency')
plt.show()

print("Data Types:")
print(df.dtypes)

# Correlation matrix
numeric_data = df.select_dtypes(include=['number'])
correlation_matrix = numeric_data.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

# Add new feature columns
df['duplicate_posts'] = np.random.randint(0, 10, size=len(df))
df['total_posts'] = df['statuses_count']

# Feature engineering
df['bio_length'] = df['description'].apply(lambda x: len(str(x)))
df['profile_picture_present'] = df['description'].apply(lambda x: 1 if x != 'default.jpg' else 0)
df['default_profile_picture'] = df['description'].apply(lambda x: 1 if x == 'default.jpg' else 0)
df['average_posts_per_day'] = df['statuses_count'] / (df['account_age'] + 1)
df['hour_of_day'] = df['created_at'].dt.hour
df['peak_activity_hour'] = df.groupby('id')['hour_of_day'].transform(lambda x: x.mode()[0])
df['account_creation_date'] = pd.to_datetime(df['created_at'], errors='coerce')
df['account_creation_year'] = df['account_creation_date'].dt.year
df['friends_to_followers_ratio'] = df['friends_count'] / (df['followers_count'] + 1)
df['spam_score'] = df['duplicate_posts'] / (df['total_posts'] + 1)

# Define features and target variable
features = [
    'account_age', 'bio_length', 'profile_picture_present', 'default_profile_image',
    'average_posts_per_day', 'peak_activity_hour', 'account_creation_year',
    'friends_to_followers_ratio','spam_score'
]
X = df[features]
y = df['is_fake']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Scale numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Ensure alignment of X_train_scaled and y_train
min_length = min(X_train_scaled.shape[0], y_train.shape[0])
X_train_scaled = X_train_scaled[:min_length]
y_train = y_train[:min_length]

# Handle missing values
imputer = SimpleImputer(strategy='mean')
X_train_scaled = imputer.fit_transform(X_train_scaled)
X_test_scaled = imputer.transform(X_test_scaled)

# Train Logistic Regression model
log_reg = LogisticRegression(class_weight='balanced', random_state=42)
log_reg.fit(X_train_scaled, y_train)

# Evaluate on test set
y_pred_log_reg = log_reg.predict(X_test_scaled)
print("Logistic Regression Classification Report:")
print(classification_report(y_test, y_pred_log_reg))
print("ROC-AUC Score:", roc_auc_score(y_test, log_reg.predict_proba(X_test_scaled)[:, 1]))

# Train Random Forest model
rf = RandomForestClassifier(class_weight='balanced', random_state=42)
rf.fit(X_train_scaled, y_train)

# Evaluate on test set
y_pred_rf = rf.predict(X_test_scaled)
print("Random Forest Classification Report:")
print(classification_report(y_test, y_pred_rf))
print("ROC-AUC Score:", roc_auc_score(y_test, rf.predict_proba(X_test_scaled)[:, 1]))

# Train XGBoost model
xgb = XGBClassifier(scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]), random_state=42)
xgb.fit(X_train_scaled, y_train)

# Evaluate on test set
y_pred_xgb = xgb.predict(X_test_scaled)
print("XGBoost Classification Report:")
print(classification_report(y_test, y_pred_xgb))
print("ROC-AUC Score:", roc_auc_score(y_test, xgb.predict_proba(X_test_scaled)[:, 1]))

# Define parameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'class_weight': ['balanced']
}

# Perform Grid Search
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='f1', n_jobs=-1)
grid_search.fit(X_train_scaled, y_train)

# Best parameters and model
print("Best Parameters:", grid_search.best_params_)
best_rf = grid_search.best_estimator_

# Evaluate best model
y_pred_best_rf = best_rf.predict(X_test_scaled)
print("Best Random Forest Classification Report:")
print(classification_report(y_test, y_pred_best_rf))
print("ROC-AUC Score:", roc_auc_score(y_test, best_rf.predict_proba(X_test_scaled)[:, 1]))

# Save the trained model
joblib.dump(best_rf, "model.pkl")

# Save the scaler (if used for preprocessing)
joblib.dump(scaler, "scaler.pkl")

from google.colab import files
files.download("model.pkl")
files.download("scaler.pkl")